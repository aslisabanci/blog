<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Blog by Aslı Sabancı]]></title><description><![CDATA[Atoms with consciousness, matter with curiosity]]></description><link>https://aslisabanci.github.io/blog</link><generator>GatsbyJS</generator><lastBuildDate>Fri, 07 Jan 2022 01:01:11 GMT</lastBuildDate><item><title><![CDATA[Generating random Turkish first names with a super basic RNN]]></title><description><![CDATA[Inspired by the mini 'Dinosaur name generator' project I did a while ago, I wanted to re-use the core code (with some modifications and refactorings of my own) and try to generate Turkish first names. The project was about building a simple recurrent neural network using numpy and it was a part of the Sequence Models course of Andrew Ng's awesome Deep Learning Specialization series on Coursera.]]></description><link>https://aslisabanci.github.io/blog/posts/generating-random-turkish-names-with-rnn</link><guid isPermaLink="false">https://aslisabanci.github.io/blog/posts/generating-random-turkish-names-with-rnn</guid><pubDate>Wed, 06 Jun 2018 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;Inspired by the mini “Dinosaur name generator” project I did a while ago, I wanted to re-use the core code (with some modifications and refactorings of my own) and try to generate Turkish first names. The project was about building a simple recurrent neural network using numpy and it was a part of the Sequence Models course of &lt;a href=&quot;http://deeplearning.ai/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Andrew Ng’s awesome Deep Learning Specialization series on Coursera&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;main-flow&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-flow&quot; aria-label=&quot;main flow permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Flow&lt;/h2&gt;
&lt;p&gt;The project builds a simple character level language model to generate new first names. Roughly, the steps are to:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the weights&lt;/li&gt;
&lt;li&gt;Forward propagate and calculate the loss&lt;/li&gt;
&lt;li&gt;Backward propagate and calculate the gradients, using the loss&lt;/li&gt;
&lt;li&gt;Clip the gradients with a max value, so that they don’t explode&lt;/li&gt;
&lt;li&gt;Update the weights using the gradients&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The input we provide is the list of characters of each name in the dataset; and the output at every time step is the next character to be used. At each time step, our recurrent neural network is picking a character, given the previous character, according to a probability distribution.&lt;/p&gt;
&lt;h2 id=&quot;preprocessing-the-data&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#preprocessing-the-data&quot; aria-label=&quot;preprocessing the data permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preprocessing the data&lt;/h2&gt;
&lt;p&gt;I wanted to try this model with two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get the most common 10 thousand names within the whole dataset (so that the dataset consists of unique names)&lt;/li&gt;
&lt;li&gt;Get 3 million names, regardless of their frequency (so that the frequent names appear more in the dataset)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For both approaches, I needed to clean the data first because some of the names contained characters like dot, paranthese, hyphen, number and what not.&lt;/p&gt;
&lt;p&gt;Also there were some non-Turkish names in the dataset, so I removed names that contain “x”, “w”, “q” as these letters are not in the Turkish alphabet.&lt;/p&gt;
&lt;p&gt;Lastly, there were a lot of first names consisting of two or three names. Since I didn’t want my model to generate names consisting of two or more names, I split these into single ones. While procesing the names one by one, I added a split name into my list of names, if it hasn’t been added in the earlier iterations.&lt;/p&gt;
&lt;p&gt;There were a lot of names with this style within my 10K and 3m dataset. So after reducing them to single names, I only ended up with ~700 and ~5000 distinct names respectively.&lt;/p&gt;
&lt;h2 id=&quot;getting-predictions&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#getting-predictions&quot; aria-label=&quot;getting predictions permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting predictions&lt;/h2&gt;
&lt;p&gt;I wanted to see how familiar the names will look, when trained with the most common Turkish names (10K dataset) and when trained with most of the names coming from the longer tail (3m dataset).&lt;/p&gt;
&lt;p&gt;So I took 10 different output samples from each model.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;10K dataset&lt;/th&gt;
&lt;th&gt;3m dataset&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sin, Badir, Sedef, Akmul, Salan, Betad, Behan, Siga, Kanefi, Sac&lt;/td&gt;
&lt;td&gt;Bevit, Ezer, Kumul, Zarit, Semiye, Gursap, Hukdeye, Sevar, Tahiye, Ozar&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vuran, Bahir, Urdi, Akir, Sam, Curec, Celkemi, Sula, Kari, Saba&lt;/td&gt;
&lt;td&gt;Goley, Niyur, Menan, Apmat, Mindorlah, Azda, Sulki, Guka, Kime, Ergi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Radugi, Dukdur, Maha, Zusuf, Mayih, Nefah, Aysu, Selsem, Gabsun, Kahat&lt;/td&gt;
&lt;td&gt;Zuraye, Rettime, Sengelli, Gulcime, Sortedi, Sugon, Zaki, Memra, Kunni, Hucan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bercan, Habbat, Nultan, Ayiz, Yurev, Havgi, Bakes, Yaydil, Melif, Kalis&lt;/td&gt;
&lt;td&gt;Gungu, Aybeden, Yavrun, Samine, Ruydar, Arnur, Hazti, Vuteton, Cahan, Talize&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Baytul, Sicen, Ekep, Ale, Murgal, Kerif, Furem, Zihan, Kubza, Luse&lt;/td&gt;
&lt;td&gt;Tekayu, Cabilatviye, Feyhan, Badabed, Sasir, Gugiki, Guhteviz, Sehhun, Sakser, Edana&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Becer, Hase, Gahat, Betri, Sidar, Erzan, Gulsul, Benan, Kevay, Efere&lt;/td&gt;
&lt;td&gt;Uzatil, Iren, Buhneyan, Sefdan, Vezhap, Fimnan, Bikan, Hirdar, Havdin, Hadrure&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Suray, Bali, Guhan, Sele, Bezih, Zaza, Zacet, Saya, Dasen, Sevdam&lt;/td&gt;
&lt;td&gt;Kurt, Iyarlan, Kagiy, Kiysan, Samay, Bervan, Safi, Fur, Zetir, Biheti&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Beyfa, Fahat, Dula, Zatkun, Semelten, Gondur, Gulerun, Buysut, Gukpek, Alpfehatengel&lt;/td&gt;
&lt;td&gt;Gocdan, Buhac, Sabip, Belet, Iben, Buhise, Aran, Semum, Sencin, Ceni&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Aseye, Emra, Fera, Oyas, Zumul, Azel, Fular, Guli, Saner, Fasi&lt;/td&gt;
&lt;td&gt;Urman, Erikmar, Buhan, Rahif, Hadul, Rosen, Daban, Hana, Rani, Zizasel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cevli, Seylut, Sayih, Gece, Kaki, Dilsin, Semi, Aser, Sabil, Boylu&lt;/td&gt;
&lt;td&gt;Heri, Soket, Serme, Selzikcan, Nurlay, Harfir, Seyyulga, Zeyar, Dilal, Gunsan&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;my-thoughts&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#my-thoughts&quot; aria-label=&quot;my thoughts permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My thoughts&lt;/h2&gt;
&lt;p&gt;For me, the results when the model is trained with the 10K dataset sound more familiar. Probably that’s because I only heard a certain amount of Turkish names in my lifetime and when looking at the whole dataset, there are many many names which feel weird / funny to me :) Naturally, these types of weird sounding Turkish names are more present in the 3m dataset, so what my model generates may sound more like them.&lt;/p&gt;
&lt;p&gt;Also what made me happy is that all the generated names are at least pronouncible in Turkish. Normally in Turkish, no more than 2 consonants appear next to each other. A vowel is usually followed by a consonant and a consonant is usually followed by another consonant or a vowel. The model seems to have followed this rule with a high percentage of the names it generated so I’m at least interpreting this as a good result.&lt;/p&gt;
&lt;p&gt;And the most fun part of this doing this project was to loudly laugh at the funny names in the dataset or the names my model predicted. So at the very least, having a good laugh was worth the time I spent on this :]&lt;/p&gt;
&lt;p&gt;If you’re curious about the implementation, head over to &lt;a href=&quot;https://github.com/aslisabanci/TurkishNameEngine&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my repo on Github&lt;/a&gt; and check out the details✌️&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Enrolling in Andrew Ng's Deep Learning Specialization MOOC for free]]></title><description><![CDATA[This year, I enrolled in Andrew Ng's awesome Deep Learning Specialization series consisting of 5 different courses on Coursera; all for free. While chatting with people about this, I realized that people usually don't know to what extend you can make use of the course material when you enroll for free. So I wanted to share my experiences here and I hope it helps someone.]]></description><link>https://aslisabanci.github.io/blog/posts/enrolling-in-coursera-for-free</link><guid isPermaLink="false">https://aslisabanci.github.io/blog/posts/enrolling-in-coursera-for-free</guid><pubDate>Sun, 06 May 2018 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;This year, I enrolled in &lt;a href=&quot;http://deeplearning.ai&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Andrew Ng’s awesome Deep Learning Specialization series&lt;/a&gt; consisting of 5 different courses on Coursera; all for free. While chatting with people about this, I realized that people usually don’t know to what extend you can make use of the course material when you enroll for free. So I wanted to share my experiences here and I hope it helps someone.&lt;/p&gt;
&lt;h2 id=&quot;videos&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#videos&quot; aria-label=&quot;videos permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Videos&lt;/h2&gt;
&lt;p&gt;You have access to all the videos and their transcripts, even after the course ends.&lt;/p&gt;
&lt;h2 id=&quot;quizzes&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#quizzes&quot; aria-label=&quot;quizzes permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quizzes&lt;/h2&gt;
&lt;p&gt;You can submit your answers to the quizzes but you can’t see the correct answers and how well you did.&lt;/p&gt;
&lt;h2 id=&quot;projects&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#projects&quot; aria-label=&quot;projects permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h2&gt;
&lt;p&gt;You have all the access to the notebooks and all the files that the notebook requires to run properly. You can download these onto your local machine and run them locally if you wish too. (This was useful for me whenever I wanted to work on my projects at a time/place without any internet connection.)
You cannot access the project notebooks after the course ends though. So either finish your projects on time, or be prepared and download all the necessary files on your machine for later access.&lt;/p&gt;
&lt;h2 id=&quot;getting-graded&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#getting-graded&quot; aria-label=&quot;getting graded permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting graded&lt;/h2&gt;
&lt;p&gt;Of course you cannot submit your project notebooks and get graded. However, the team behind this MOOC has prepared all the notebooks in a way that they guide you and then they tell you the expected results of the step you’re implementing. Seeing the expected results after every step of your implementation is very helpful because you can spot your mistakes early instead of trying to understand what is wrong with your code; only at the end.&lt;/p&gt;
&lt;p&gt;So if you can get all the expected results with your implementation, you get your feedback that your solution gets the job done and you feel like you got a good grade :]&lt;/p&gt;
&lt;h2 id=&quot;certification&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#certification&quot; aria-label=&quot;certification permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certification&lt;/h2&gt;
&lt;p&gt;Since you don’t get graded, you don’t expect to get a certificate either.&lt;/p&gt;
&lt;p&gt;So, to wrap up: If your only aim is to learn and experiment; enrolling for free can get you almost all the benefits in my opinion. The “expected results” on the notebooks give you the necessary feedback for your projects and you can get your hands on all the materials. If getting a certificate is not important for you; enrolling for free could you be your feasible solution to enjoy all this fantastic material.&lt;/p&gt;
&lt;p&gt;Don’t forget to praise this awesome course at every opportunity though :] because the effort behind all that material is huge and Andrew Ng is a great tutor in my opinion.&lt;/p&gt;
&lt;p&gt;Let me know if you have any questions and I’d be more than happy to help!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Kaggle Munich Meetup]]></title><description><![CDATA[Yesterday was my third week in Munich and to join the community of the city's machine learning peeps, I went to Kaggle Munich's meetup, held at the Google office.]]></description><link>https://aslisabanci.github.io/blog/posts/kaggle-munich-meetup</link><guid isPermaLink="false">https://aslisabanci.github.io/blog/posts/kaggle-munich-meetup</guid><pubDate>Sun, 06 May 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Yesterday was my third week in Munich and to join the community of the city’s machine learning peeps, I went to &lt;a href=&quot;https://www.meetup.com/Kaggle-Munich/events/past/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Kaggle Munich’s meetup&lt;/a&gt;, held at the Google office.&lt;/p&gt;
&lt;p&gt;The agenda was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Listen to two talks&lt;/li&gt;
&lt;li&gt;Chat with people in the breaks&lt;/li&gt;
&lt;li&gt;Do some hands-on Kaggle hacking either by yourself, but preferably by creating a small team&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;talks&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#talks&quot; aria-label=&quot;talks permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Talks&lt;/h2&gt;
&lt;p&gt;The first talk was about the Vectorflow framework, which is a lightweight neural network framework implemented in the D language; and optimized for processing sparse data. Maybe someday I’ll get my hands on this while dabbling in a Kaggle competition - I’d like to see what advantages it would bring me.&lt;/p&gt;
&lt;p&gt;The second talk was about Hierarchical Temporal Memory, which is an experimental approach to machine learning where the neurons are different than the usual neural network neurons most of us are used to. This approach is inspired by our neocortex and it could have some advantages for certain types of problems. I don’t think I will get my hands on this in the short term, but still, it was interesting to learn about this new way of thinking. The slides of this talk are &lt;a href=&quot;https://github.com/ConnorJL/Kaggle-Munich-HTM&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;here on github&lt;/a&gt;, if you’d like to check them out.&lt;/p&gt;
&lt;h2 id=&quot;hands-on-kaggling&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#hands-on-kaggling&quot; aria-label=&quot;hands on kaggling permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands on Kaggling&lt;/h2&gt;
&lt;p&gt;Although two of the talks were engaging, the most fun part of the meetup was the hands on Kaggle hacking session. I was curious about this &lt;a href=&quot;https://www.kaggle.com/c/avito-demand-prediction&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Avito Demand Prediction Challenge competition&lt;/a&gt; so I started a team, by acting as the noob seed and then we became a small team of three, when two other Kagglers joined me. We started by taking a look at the existing kernels and checked out some exploratory data analysis posts and some model implementations. We then submitted a super basic prediction output - just to have a baseline result to improve later on :]&lt;/p&gt;
&lt;h2 id=&quot;verdict&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#verdict&quot; aria-label=&quot;verdict permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Verdict😛&lt;/h2&gt;
&lt;p&gt;All in all, this was a super fun and friendly meetup. I am looking forward to the next one and I absolutely recommend it to you too, if you happen to be around Munich at that time and are keen to spend a good 3-4 hours thinking of and with data.&lt;/p&gt;</content:encoded></item></channel></rss>